{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline de extracción, este es el bueno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to\n",
      "[nltk_data]     C:\\Users\\Maracuya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os \n",
    "\n",
    "nltk.download('cess_esp')\n",
    "from nltk.corpus import cess_esp\n",
    "#Por mejorar, este corpus está chiquito\n",
    "spanish_words = set(word for word in cess_esp.words())\n",
    "\n",
    "path = \"data/\"\n",
    "documents = []\n",
    "\n",
    "# Get all the PDF documents NAMES from the path\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        documents.append(file[:-4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_text_from_pdf(key):\n",
    "    doc = fitz.open(f'data/{key}.pdf')\n",
    "    number_of_pages = doc.page_count\n",
    "\n",
    "    i=documents[key][0]\n",
    "    full_text=\"\"\n",
    "    for page in doc.pages(documents[key][0], number_of_pages-documents[key][1]):\n",
    "        # print(\"Page number: \", i)\n",
    "        this_page_text=page.get_text(\"text\", sort=True).replace(\"\\n\", \" \")\n",
    "        this_page_text=re.sub(r'\\d+', '', this_page_text)\n",
    "        full_text+=this_page_text+\" \"\n",
    "        i+=1\n",
    "    return full_text\n",
    "\n",
    "def get_all_text_from_pdf(key):\n",
    "    doc = fitz.open(f'data/{key}.pdf')\n",
    "    full_text=\" \".join([page.get_text(\"text\", sort=True).replace(\"\\n\", \" \") for page in doc])\n",
    "    full_text=re.sub(r'\\d+', '', full_text)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from langdetect import detect\n",
    "import langid\n",
    "\n",
    "def filtrar_espaniol(full_text):\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    non_spanish_sentences = []\n",
    "\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            languages=[]\n",
    "            languages.append(detect(sentence))\n",
    "            languages.append(langid.classify(sentence)[0])\n",
    "            if not any(lang in languages for lang in [\"es\", \"it\", \"ca\", \"pt\"]):\n",
    "                non_spanish_sentences.append(sentence)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    sentences = non_spanish_sentences\n",
    "    \n",
    "\n",
    "    return sentences\n",
    "\n",
    "def filtrar_oracion_espaniol(sentence):\n",
    "    try:\n",
    "        languages=[]\n",
    "        languages.append(detect(sentence))\n",
    "        languages.append(langid.classify(sentence)[0])\n",
    "        if not any(lang in languages for lang in [\"es\", \"it\", \"ca\", \"pt\"]):\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones de filtrado\n",
    "alfabeto_quechua = ['a', 'aa', 'ch', 'chh', 'ch\\'', 'ts', 'tr', 'h', 'i', 'ii', 'k', 'kh', 'k\\'', 'l', 'll', 'm', 'n', 'ñ', 'p', 'ph', 'p\\'', 'q', 'qh', 'q\\'', 'r', 's', 'sh', 't', 'th', 't\\'', 'u', 'uu', 'w', 'y']\n",
    "\n",
    "def grafemas_no_en_alfabet(words):\n",
    "    for word in words:\n",
    "        for i, letter in enumerate(word):\n",
    "            #Continue  if letter is not a letter\n",
    "            if not letter.isalpha():\n",
    "                continue\n",
    "            if letter.lower() not in alfabeto_quechua:\n",
    "                #Chequear siguiente letra\n",
    "                if i+1 >= len(word):\n",
    "                    return False\n",
    "                letter = letter + word[i+1]\n",
    "                if letter.lower() not in alfabeto_quechua:\n",
    "                    if i+2 >= len(word):\n",
    "                        return False\n",
    "                    #Chequear siguiente letra\n",
    "                    letter = letter + word[i+2]\n",
    "                    if letter.lower() not in alfabeto_quechua:\n",
    "                        return False\n",
    "    return True\n",
    "\n",
    "#No funciona bien esta función, porque el detector de lenguaje no es muy bueno para palabras \n",
    "def oraciones_mucho_espaniol(words):\n",
    "    spanish_words = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            if detect(word) == 'es':\n",
    "                spanish_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    return spanish_words/len(words) > 0.5\n",
    "\n",
    "def oracion_mucho_espaniol_v2(words):\n",
    "    palabras_encontradas = 0\n",
    "    #Usar un diccionario de palabras en español\n",
    "    for word in words:\n",
    "        if word.lower() in spanish_words:\n",
    "            palabras_encontradas += 1\n",
    "\n",
    "    return palabras_encontradas/len(words) < 0.25\n",
    "\n",
    "\n",
    "def oraciones_muy_cortas(words, min_length=3):\n",
    "    return len(words) > min_length\n",
    "\n",
    "def oraciones_muy_repititivas(words, threshold=0.4):\n",
    "    unique_words = set(words)\n",
    "    ratio = len(unique_words) / len(words)\n",
    "    return ratio >= threshold\n",
    "\n",
    "def palabras_muy_largas(words, threshold=40):\n",
    "    for word in words:\n",
    "        if len(word) > threshold:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def split_tokens(sentence):\n",
    "    #Esta expresión busca cualquier secuencia de tres o más palabras que tengan cada una uno o dos caracteres de longitud, separadas por espacios.\n",
    "    #En el paper se indica que es para detectar partes de una palabra que se rompieron durante la extracción de texto debido al formato del pdf.\n",
    "    if re.search(r\"(\\b\\w{1,2}\\b\\s){3,}\", sentence):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def oraciones_con_matematica(sentence):\n",
    "    if re.search(r\"[\\d+\\-*/]+\", sentence):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a, aa, ch, chh, ch', ts, tr, h, i, ii, k, kh, k', l, ll, m, n, ñ, p, ph, p', q, qh, q', r, s, sh, t, th, t', u, uu, w, y\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Reglas basadas en https://aclanthology.org/2020.lrec-1.356/\n",
    "def rule_based_heuristic(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    aux_solo_palabras = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    valid = oraciones_muy_cortas(aux_solo_palabras) \n",
    "    valid = valid and oraciones_muy_repititivas(aux_solo_palabras)\n",
    "    valid = valid and palabras_muy_largas(words)\n",
    "    valid = valid and split_tokens(sentence)\n",
    "    valid = valid and oraciones_con_matematica(sentence)\n",
    "    valid = valid and oracion_mucho_espaniol_v2(aux_solo_palabras)\n",
    "\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(documents):\n",
    "    corpus=pd.DataFrame(columns=[\"document\", \"sentence\"])\n",
    "    for key in documents:\n",
    "        text=get_all_text_from_pdf(key)\n",
    "        sentences=sent_tokenize(text)\n",
    "        #Oraciones antes de filtrar\n",
    "        print(\"Oraciones antes de filtrar en documento \", key, \": \", len(sentences))\n",
    "        filtered_sentences=[]\n",
    "        for sentence in sentences:\n",
    "            if rule_based_heuristic(sentence) and filtrar_oracion_espaniol(sentence):\n",
    "                filtered_sentences.append(sentence)\n",
    "        \n",
    "        df=pd.DataFrame(filtered_sentences, columns=[\"sentence\"])\n",
    "        df[\"document\"]=key\n",
    "        #Quitar duplicados\n",
    "        df.drop_duplicates(subset=\"sentence\", inplace=True)\n",
    "        print(\"Oraciones después de filtrar en documento \", key, \": \", len(df))\n",
    "        corpus=pd.concat([corpus, df], ignore_index=True)\n",
    "    print(\"Total de oraciones: \", len(corpus))\n",
    "    corpus.drop_duplicates(subset=\"sentence\", inplace=True)\n",
    "    print(\"Total de oraciones únicas: \", len(corpus))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus=pipeline(documents)\n",
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus.loc[df_corpus[\"sentence\"].apply(lambda x: len(x))<40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus.to_csv(\"data/corpus/avanceHarvy.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
