{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intento con .txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se uso Acrobat para exportar el pdf a formato .txt\n",
    "\n",
    "https://repositorio.minedu.gob.pe/browse?value=Quechua+Chanka&type=subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amy.ipynb\n",
      "CuadernoPrincipal.ipynb\n",
      "Harvy.ipynb\n",
      "README.md\n",
      "Renzo.ipynb\n",
      "data\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#os.chdir(\"./data/txt\")\n",
    "#print(os.getcwd())\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amy.ipynb\n",
      "CuadernoPrincipal.ipynb\n",
      "Harvy.ipynb\n",
      "README.md\n",
      "Renzo.ipynb\n",
      "data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key=\"2 Rimana - Qillqasqa maytu - Qichwa, Chanka. Texto de Comunicación del 2° Secundaria - Quechua chanka\"\n",
    "key=\"Anqarakunapa kawsakuyninmanta. Literatura 2 - 4° Primaria - Quechua chanka\"\n",
    "\n",
    "path = f'./data/txt/{key}.txt'\n",
    "\n",
    "try:\n",
    "    with open(path, 'r', encoding='ISO-8859-1') as file:\n",
    "        text = file.read()\n",
    "except UnicodeDecodeError:\n",
    "    with open(path, 'r', encoding='Windows-1252') as file:\n",
    "        text = file.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Eliminar saltos de línea y tabulaciones\n",
    "text = text.replace('\\n', ' ').replace('\\r', '').replace('\\t', ' ')\n",
    "\n",
    "# Opcional: Eliminar caracteres no deseados\n",
    "text = re.sub(r'[^a-zA-Z0-9.,;:!?() ]', '', text)\n",
    "\n",
    "# Eliminar espacios extra\n",
    "text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Pasar a minúsculas\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palabras: 14938\n",
      "Número de frases: 1008\n",
      "Las diez palabras más comunes: [('de', 586), ('los', 392), ('y', 316), ('la', 294), ('en', 253), ('el', 252), ('que', 183), ('con', 159), ('a', 155), ('se', 143)]\n"
     ]
    }
   ],
   "source": [
    "# Contar palabras\n",
    "words = text.split()\n",
    "num_words = len(words)\n",
    "print(\"Número de palabras:\", num_words)\n",
    "\n",
    "# Contar frases (asumiendo que cada punto final representa el fin de una frase)\n",
    "sentences = text.split('.')\n",
    "num_sentences = len(sentences)\n",
    "print(\"Número de frases:\", num_sentences)\n",
    "\n",
    "# Frecuencia de palabras\n",
    "from collections import Counter\n",
    "frequency = Counter(words)\n",
    "print(\"Las diez palabras más comunes:\", frequency.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrado de oraciones en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\renzo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oraciones antes de filtrar:  996\n",
      "Oraciones después de filtrar:  473\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from langdetect import detect\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "non_spanish_sentences = []\n",
    "\n",
    "#Oraciones antes de filtrar\n",
    "print(\"Oraciones antes de filtrar: \", len(sentences))\n",
    "for sentence in sentences:\n",
    "    try:\n",
    "        if detect(sentence) != 'es':\n",
    "            non_spanish_sentences.append(sentence)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "sentences = non_spanish_sentences\n",
    "print(\"Oraciones después de filtrar: \", len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las oraciones no españolas en un archivo\n",
    "with open(f'./data/corpus/{key}.txt', 'w', encoding='utf-8') as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a, aa, ch, chh, ch', ts, tr, h, i, ii, k, kh, k', l, ll, m, n, ñ, p, ph, p', q, qh, q', r, s, sh, t, th, t', u, uu, w, y\n",
    "alfabeto_quechua = ['a', 'aa', 'ch', 'chh', 'ch\\'', 'ts', 'tr', 'h', 'i', 'ii', 'k', 'kh', 'k\\'', 'l', 'll', 'm', 'n', 'ñ', 'p', 'ph', 'p\\'', 'q', 'qh', 'q\\'', 'r', 's', 'sh', 't', 'th', 't\\'', 'u', 'uu', 'w', 'y']\n",
    "\n",
    "def rule_based_heuristic(sentence):\n",
    "    #exclude sentences containing words formed by graphemes outside the alphabet\n",
    "    words = word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        if word not in alfabeto_quechua:\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Con PDF (copia Harvy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "import fitz\n",
    "import pandas as pd\n",
    "\n",
    "documents={\n",
    "    \"2 Rimana - Qillqasqa Mayt’u Qichwa Qullaw. Texto de Comunicación del 2° Secundaria - Quechua Collao\": (4, 6),\n",
    "    #\"Anqarakunapa kawsakuyninmanta. Literatura 2 - 4° Primaria - Quechua chanka\": (5,6),\n",
    "    #\"Antuku hampatuwan. Historias y relatos 2- Inicial - Quechua chanka\": (4, 3),\n",
    "    \"Ayllupi yachasunchik 3 ñiqi Qullaw qichwapi. Texto de Comunicación del 3° Primaria - Quechua Collao\": (4,3),\n",
    "    \"Huk kutis kaq kasqa Literatura 2 quechua collao\": (4,5),\n",
    "    \"Huk kutis kaq kasqa. Literatura 1 - 5° Primaria - Quechua collao\": (4,5),\n",
    "    \"Llaqtanchikpa kawsayninkuna Saberes de los pueblos 2 - 2° Secundaria - Quechua collao\": (4, 5),\n",
    "    \"Llaqtanchikpa kawsayninkuna. Saberes de los pueblos 1 - 1° Secundaria - Quechua collao\": (4, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_text_from_pdf(key):\n",
    "    doc = fitz.open(f'data/{key}.pdf')\n",
    "    number_of_pages = doc.page_count\n",
    "\n",
    "    i=documents[key][0]\n",
    "    full_text=\"\"\n",
    "    for page in doc.pages(documents[key][0], number_of_pages-documents[key][1]):\n",
    "        # print(\"Page number: \", i)\n",
    "        this_page_text=page.get_text(\"text\", sort=True).replace(\"\\n\", \" \")\n",
    "        this_page_text=re.sub(r'\\d+', '', this_page_text)\n",
    "        full_text+=this_page_text+\" \"\n",
    "        i+=1\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from langdetect import detect\n",
    "import langid\n",
    "\n",
    "def filtrar_espaniol(full_text):\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    non_spanish_sentences = []\n",
    "\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            languages=[]\n",
    "            languages.append(detect(sentence))\n",
    "            languages.append(langid.classify(sentence)[0])\n",
    "            if not any(lang in languages for lang in [\"es\", \"it\", \"ca\", \"pt\"]):\n",
    "                non_spanish_sentences.append(sentence)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    sentences = non_spanish_sentences\n",
    "    \n",
    "\n",
    "    return sentences\n",
    "\n",
    "def filtrar_oracion_espaniol(sentence):\n",
    "    try:\n",
    "        languages=[]\n",
    "        languages.append(detect(sentence))\n",
    "        languages.append(langid.classify(sentence)[0])\n",
    "        if not any(lang in languages for lang in [\"es\", \"it\", \"ca\", \"pt\"]):\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones de filtrado\n",
    "alfabeto_quechua = ['a', 'aa', 'ch', 'chh', 'ch\\'', 'ts', 'tr', 'h', 'i', 'ii', 'k', 'kh', 'k\\'', 'l', 'll', 'm', 'n', 'ñ', 'p', 'ph', 'p\\'', 'q', 'qh', 'q\\'', 'r', 's', 'sh', 't', 'th', 't\\'', 'u', 'uu', 'w', 'y']\n",
    "\n",
    "def grafemas_no_en_alfabet(words):\n",
    "    for word in words:\n",
    "        for i, letter in enumerate(word):\n",
    "            #Continue  if letter is not a letter\n",
    "            if not letter.isalpha():\n",
    "                continue\n",
    "            if letter.lower() not in alfabeto_quechua:\n",
    "                #Chequear siguiente letra\n",
    "                if i+1 >= len(word):\n",
    "                    return False\n",
    "                letter = letter + word[i+1]\n",
    "                if letter.lower() not in alfabeto_quechua:\n",
    "                    if i+2 >= len(word):\n",
    "                        return False\n",
    "                    #Chequear siguiente letra\n",
    "                    letter = letter + word[i+2]\n",
    "                    if letter.lower() not in alfabeto_quechua:\n",
    "                        return False\n",
    "    return True\n",
    "\n",
    "def oraciones_mucho_espaniol(words):\n",
    "    spanish_words = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            if detect(word) == 'es':\n",
    "                spanish_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    return spanish_words/len(words) > 0.5\n",
    "\n",
    "def oraciones_muy_cortas(words, min_length=3):\n",
    "    #Conservar solo palabras que no sean puntuación\n",
    "    aux = [word for word in words if word.isalpha()]\n",
    "    return len(aux) > min_length\n",
    "\n",
    "def oraciones_muy_repititivas(words, threshold=0.4):\n",
    "    unique_words = set(words)\n",
    "    ratio = len(unique_words) / len(words)\n",
    "    return ratio >= threshold\n",
    "\n",
    "def palabras_muy_largas(words, threshold=40):\n",
    "    for word in words:\n",
    "        if len(word) > threshold:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def split_tokens(sentence):\n",
    "    # Check for three or more sequential tokens composed of one or two characters\n",
    "    if re.search(r\"(\\b\\w{1,2}\\b\\s){3,}\", sentence):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def oraciones_con_matematica(sentence):\n",
    "    if re.search(r\"[\\d+\\-*/]+\", sentence):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a, aa, ch, chh, ch', ts, tr, h, i, ii, k, kh, k', l, ll, m, n, ñ, p, ph, p', q, qh, q', r, s, sh, t, th, t', u, uu, w, y\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Reglas basadas en https://aclanthology.org/2020.lrec-1.356/\n",
    "def rule_based_heuristic(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    valid = oraciones_muy_cortas(words) \n",
    "    valid = valid and oraciones_muy_repititivas(words)\n",
    "    valid = valid and palabras_muy_largas(words)\n",
    "    valid = valid and split_tokens(sentence)\n",
    "    valid = valid and oraciones_con_matematica(sentence)\n",
    "\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(documents):\n",
    "    corpus=pd.DataFrame(columns=[\"document\", \"sentence\"])\n",
    "    for key, value in documents.items():\n",
    "        text=get_text_from_pdf(key)\n",
    "        sentences=sent_tokenize(text)\n",
    "        #Oraciones antes de filtrar\n",
    "        print(\"Oraciones antes de filtrar en documento \", key, \": \", len(sentences))\n",
    "        filtered_sentences=[]\n",
    "        for sentence in sentences:\n",
    "            if rule_based_heuristic(sentence) and filtrar_oracion_espaniol(sentence):\n",
    "                filtered_sentences.append(sentence)\n",
    "        \n",
    "        df=pd.DataFrame(filtered_sentences, columns=[\"sentence\"])\n",
    "        df[\"document\"]=key\n",
    "        #Quitar duplicados\n",
    "        df.drop_duplicates(subset=\"sentence\", inplace=True)\n",
    "        print(\"Oraciones después de filtrar en documento \", key, \": \", len(df))\n",
    "        corpus=pd.concat([corpus, df], ignore_index=True)\n",
    "    print(\"Total de oraciones: \", len(corpus))\n",
    "    corpus.drop_duplicates(subset=\"sentence\", inplace=True)\n",
    "    print(\"Total de oraciones únicas: \", len(corpus))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus=pipeline(documents)\n",
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show me the sortest sentences  content\n",
    "df_corpus.loc[df_corpus[\"sentence\"].apply(lambda x: len(x))<40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus.to_csv(\"data/corpus/avances/avanceRenzo.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
